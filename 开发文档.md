AI英语朗读纠音助手 - 完整开发文档
版本: 1.0
编制人: Gemini (资深编程与教学专家)

1. 项目概述与核心目标
1.1. 项目简介
本项目旨在开发一款AI辅助工具，服务于小学纠音老师。该工具通过自动化分析学生朗读的音频/视频，智能识别发音错误、错词、漏词等问题，并提供备课支持与针对性反馈，从而极大提升老师的工作效率，并为学生提供即时、准确的辅导。

1.2. 核心目标

效率提升: 将老师从繁琐、重复的听读与批改工作中解放出来。

精准纠错: 利用AI技术，提供比人工更细致的音素级别发音问题诊断。

个性化辅导: 为每个学生生成专属的发音问题报告和改进建议。

教学辅助: 在备课阶段，提前为老师高亮文本中的教学难点和发音重点。

成本控制: 全部功能模块优先采用免费、开源的技术方案实现。

2. 系统架构设计
本系统采用前后端分离的架构。

前端 (Frontend): 一个纯静态的Web界面，由HTML, CSS, JavaScript构成。用户在浏览器中直接与之交互。它负责收集用户输入（文本、文件上传），向后端发送请求，并美观地展示后端返回的分析结果。

后端 (Backend): 一个在本地或服务器上运行的Python应用程序。它作为系统的大脑，负责处理所有核心逻辑，包括文本分析、音频处理、语音识别和发音评估。它通过API接口与前端进行数据交换。

AI模型/依赖: 后端程序会调用一系列开源AI模型和处理工具（如Whisper, NLTK等）来完成复杂的分析任务。这些工具需要被安装在后端运行的环境中。

数据流图:
用户操作 (前端) -> HTTP请求 (Fetch API) -> 后端API接口 (Flask) -> 调用AI模型/库处理 -> 返回JSON数据 -> 前端JS解析并更新UI

3. 技术选型 (全免费开源方案)
模块

技术/库

用途

前端

HTML5, CSS3, JavaScript (ES6+)

构建用户界面和交互逻辑。

Bootstrap 5

快速构建现代化、响应式的UI界面。

后端

Python 3.9+

后端主要开发语言。

Flask

轻量级Web框架，用于构建API接口。

Flask-CORS

解决前后端分离架构下的跨域请求问题。

文本分析

NLTK (Natural Language Toolkit)

用于分词、词性标注。

CMU Pronouncing Dictionary

查找单词的标准音标（ARPAbet）。

音视频处理

FFmpeg

从视频中无损提取音频流。

语音识别(STT)

OpenAI Whisper (Library)

将学生的朗读音频高精度地转换为文本。

文本比对

Python difflib

对比原始文本和识别文本，找出错词、漏词、增词。

数据库

SQLite3 (Python内置)

轻量级数据库，用于存储学生信息、作业历史等。

SQLAlchemy (可选，推荐)

对象关系映射(ORM)工具，简化数据库操作。

(高级)发音评估

Montreal Forced Aligner (MFA)

将音频与文本在音素层面进行强制对齐，是实现音素级发音好坏判断的核心免费工具。


导出到 Google 表格
4. 项目目录结构建议
/ai_teacher_assistant/
|
├── /backend/
|   ├── app.py             # Flask主应用文件，包含所有API接口
|   ├── models.py          # (可选)数据库模型定义 (如果使用SQLAlchemy)
|   ├── services/          # 核心服务逻辑文件夹
|   |   ├── text_analyzer.py # 文本分析模块
|   |   └── audio_corrector.py # 音频处理与纠错模块
|   ├── uploads/           # 用于临时存放上传的音频文件
|   └── requirements.txt   # Python依赖列表
|
├── /frontend/
|   ├── index.html         # 主HTML文件 (即之前的ai-teacher.html)
|   └── assets/
|       ├── css/
|       |   └── style.css  # 自定义样式
|       └── js/
|           └── main.js    # 主要的JavaScript交互逻辑
|
└── README.md              # 项目说明文档
5. 核心功能模块与API设计
模块一: 智能备课 (文本预分析)
前端界面: 一个文本输入框，一个“分析”按钮，一个用于展示分析报告的区域。

后端逻辑:

接收前端发来的文本。

使用NLTK分词。

遍历每个单词，查询CMUDict获取其音标。

根据预设的“易错音素规则库” (如 /θ/, /r/, /l/, /v/ 等)，标记出包含这些音素的单词。

根据简单的语法规则（如查找-ing, -ed, -s结尾的词），识别潜在的知识点。

将结果打包成JSON格式返回。

API接口设计:

Endpoint: POST /api/text_analysis

Request Body (JSON):

JSON

{
  "text": "Today the birds are singing."
}
Response Body (JSON):

JSON

{
  "difficult_words": {
    "the": ["包含易错音 /ð/"],
    "birds": ["包含易错音 /r/", "包含易错音 /z/"],
    "singing": ["包含易-错音 /ŋ/"]
  },
  "knowledge_points": {
    "现在进行时 (-ing)": ["singing"],
    "名词复数 (-s/-es)": ["birds"]
  }
}
模块二: 智能批改 (音视频分析)
前端界面: 一个文件上传控件（用于上传学生的音视频），一个“开始批改”按钮，一个用于展示批改报告的区域。

后端逻辑:

接收前端上传的音频文件和原始文本。

(如果是视频) 使用FFmpeg命令行工具，从视频中提取WAV或MP3格式的音频。

使用Whisper模型，将学生的音频转换为文本。

使用difflib库，对比Whisper识别的文本和原始文本，生成差异报告，定位错词、漏词、增词。

(高级功能) 将学生音频和原始文本输入到Montreal Forced Aligner。MFA会输出每个单词和音素的精确起止时间及一个置信度分数。低分数的音素/单词即为发音不准的地方。

汇总所有错误信息，计算一个综合得分，打包成JSON返回。

API接口设计:

Endpoint: POST /api/audio_correction

Request Body (Multipart Form Data):

file: 上传的音频或视频文件。

original_text: 对应的原始文本字符串。

Response Body (JSON):

JSON

{
  "overall_score": 85,
  "transcription": "Today ze birds are singin.", // Whisper识别结果
  "word_errors": [
    { "original": "the", "spoken": "ze", "type": "mispronounced" },
    { "original": "singing", "spoken": "singin", "type": "mispronounced" }
  ],
  "phoneme_errors": [ // 高级功能产出
    { "word": "the", "phoneme": "/ð/", "issue": "发成了 /z/" },
    { "word": "singing", "phoneme": "/ŋ/", "issue": "鼻音丢失" }
  ]
}
模块三: 智能辅导与数据管理
前端界面: 在报告中展示发音技巧（可以是预设的图文），并提供标准发音的播放按钮。一个学生列表和历史作业记录查看界面。

后端逻辑:

发音技巧: 这是静态内容，可以预先写好，根据检测到的错误类型（如/θ/错误）由前端直接显示对应的帮助内容。

标准发音: 前端可以使用浏览器内置的SpeechSynthesis API来朗读单词，无需后端。

数据管理: 设计API用于增、删、改、查学生信息和他们的朗读记录。

数据库表设计 (SQLite):

students Table:

id (INTEGER, PRIMARY KEY)

name (TEXT, NOT NULL)

readings Table:

id (INTEGER, PRIMARY KEY)

student_id (INTEGER, FOREIGN KEY to students.id)

original_text (TEXT)

submission_date (DATETIME)

audio_path (TEXT)

score (INTEGER)

report_json (TEXT)  // 存储模块二返回的完整JSON报告

6. 分步开发路线图 (Roadmap)
里程碑一: 环境搭建与后端基础

[ ] 安装Python, FFmpeg。

[ ] 使用pip安装Flask, flask-cors, nltk, openai-whisper等所有依赖。

[ ] 初始化项目目录结构。

[ ] 搭建基础的Flask应用，并成功与前端index.html页面建立通信（完成一个“hello world”的API即可）。

里程碑二: 实现模块一 (文本分析)

[ ] 在后端完成text_analyzer.py的逻辑。

[ ] 创建/api/text_analysis接口。

[ ] 在前端JS中，实现点击按钮后调用该接口，并动态渲染返回的分析报告。

里程碑三: 实现模块二的核心 (语音识别与文本比对)

[ ] 在后端完成audio_corrector.py的核心逻辑。

[ ] 实现文件上传接口/api/audio_correction。

[ ] 集成FFmpeg调用，用于视频转音频。

[ ] 集成Whisper，实现语音转文本。

[ ] 集成difflib，实现文本比对。

[ ] 在前端JS中，实现上传文件和文本，调用接口，并展示返回的批改报告。

里程碑四: 数据持久化

[ ] 设计并创建SQLite数据库表。

[ ] 在后端引入数据库操作逻辑（推荐使用SQLAlchemy）。

[ ] 修改批改接口，将每次的批改结果存入数据库。

[ ] 创建新的API用于查询学生列表和历史记录。

[ ] 在前端添加学生管理和历史记录查看界面。

里程碑五: (高级) 集成音素级评估

[ ] 安装并学习使用Montreal Forced Aligner (MFA)。这是一个相对复杂的步骤。

[ ] 在audio_corrector.py中，添加调用MFA的流程。

[ ] 解析MFA的输出，识别低置信度的音素作为发音错误。

[ ] 将音素级别的错误添加到API的返回结果中，并在前端展示。